---
layout: page
title: Yuekun Yao
---

## About me

  <img src="https://raw.githubusercontent.com/ykyaol7/ykyaol7.github.io/master/static/images/photo.jpg" 
       alt="photo" 
       style="float: right; margin-left: 20px; zoom: 20%;" />

Hi! I am Yuekun Yao, a Ph.D. student in [Department of Language Science and Technology](https://www.uni-saarland.de/en/department/lst.html) at [Saarland University](https://www.uni-saarland.de/en/home.html) working with Prof. [Alexander Koller](https://www.coli.uni-saarland.de/~koller/). I am a part of [Computational Linguistic Group](https://www.coli.uni-saarland.de/groups/AK/). In the past, I got my MSc degree in artificial intelligence at the University of Edinburgh. Before that, I did my BS in computer science at East China Normal University. 

The main research question I am interested in is **How does NLP models generalize to unfamiliar data and how can we improve it?** I investigate **out-of-distribution generalization** with a focus on **compositional generalization** to bridge the gap between training and test distributions in realistic applications. I am also interested in **trustworthiness** of NLP models to detect their generalization errors when deployed in real-world settings. 

My work aims to both understand model behaviours and develop more effective and reliable NLP models through the following research questions.

- Can NLP models perform human-like generalization, and why? [[1]](https://aclanthology.org/2022.emnlp-main.337/) Does this also apply to large language models? [[2]](https://aclanthology.org/2023.emnlp-main.194/)
- How to improve models' compositional generalization ability with general-purpose models (seq2seq)? [[3]](https://arxiv.org/abs/2401.09815)
- How to build trustworthy models that generalize reliably? Can we train one model (discriminator) to judge the outputs of another model (parser)? [[4]](https://arxiv.org/abs/2311.09422)



## Publications [[Google Scholar]](https://scholar.google.com/citations?user=sWCmrQEAAAAJ)[[Semantic Scholar]](https://www.semanticscholar.org/author/Yuekun-Yao/1733485928)

<small>2025</small>

<div style="margin-bottom: 1em;"> 
  <strong>Language models can learn implicit multi-hop reasoning, but only if they have lots of training data</strong> [<a href="https://www.arxiv.org/abs/2505.17923">paper</a>] <br>  
  <em>Yuekun Yao, Yupei Du, Dawei Zhu, Michael Hahn, Alexander Koller</em><br> 
  <span style="font-style: italic;">arXiv preprint arXiv:2505.17923, 2025 (Under review)</span> 
</div> 

<div style="margin-bottom: 1em;"> 
  <strong>Reason to rote: Rethinking memorization in reasoning</strong> [<a href="https://arxiv.org/abs/2507.04782">paper</a>] <br> 
  <em>Yupei Du, Philipp Mondorf, Silvia Casola, Yuekun Yao, Robert Litschko, Barbara Plank</em><br> 
  <span style="font-style: italic;">Preprint, 2025 (Under review)</span> 
</div>

<div style="margin-bottom: 1em;"> <strong>Anything Goes? A Crosslinguistic Study of (Im)possible Language Learning in LMs</strong><br> <em>Xiulin Yang, Tatsuya Aoyama, Yuekun Yao, Ethan Wilcox</em><br> <span style="font-style: italic;">Preprint, 2025</span> </div>
<small>2024</small>

<div style="margin-bottom: 1em;"> <strong>Predicting generalization performance with correctness discriminators</strong> [<a href="https://arxiv.org/abs/2311.09422">paper</a>]<br> <em>Yuekun Yao, Alexander Koller</em><br> <span style="font-style: italic;">Findings of the 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP 2024)</span> </div> <div style="margin-bottom: 1em;"> <strong>Simple and effective data augmentation for compositional generalization</strong> [<a href="https://aclanthology.org/2024.naacl-long.25/">paper</a>] [<a href="https://github.com/coli-saar/data-augmentation-compgen">code</a>]<br> <em>Yuekun Yao, Alexander Koller</em><br> <span style="font-style: italic;">The 2024 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2024)</span> </div>
<small>2023</small>

<div style="margin-bottom: 1em;"> <strong>SLOG: A Structural Generalization Benchmark for Semantic Parsing</strong> [<a href="https://aclanthology.org/2023.emnlp-main.194/">paper</a>] [<a href="https://github.com/bingzhilee/SLOG">code</a>]<br> <em>Bingzhi Li, Lucia Donatelli, Alexander Koller, Tal Linzen, Yuekun Yao, Najoung Kim</em><br> <span style="font-style: italic;">The 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP 2023)</span> </div>
<small>2022</small>

<div style="margin-bottom: 1em;"> <strong>Structural generalization is hard for sequence-to-sequence models</strong> [<a href="https://aclanthology.org/2022.emnlp-main.337/">paper</a>] [<a href="https://github.com/coli-saar/Seq2seq-on-COGS">code</a>] [<a href="https://github.com/coli-saar/Syntax-COGS">data</a>]<br> <em>Yuekun Yao, Alexander Koller</em><br> <span style="font-style: italic;">The 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP 2022)</span> </div>
<small>2020</small>

<div style="margin-bottom: 1em;"> <strong>Dynamic masking for improved stability in online spoken language translation</strong> [<a href="https://aclanthology.org/2020.amta-research.12.pdf">paper</a>]<br> <em>Yuekun Yao, Barry Haddow</em><br> <span style="font-style: italic;">The 14th Biennial Conference of the Association for Machine Translation in the Americas (AMTA 2020)</span> </div> <div style="margin-bottom: 1em;"> <strong>ELITR non-native speech translation at IWSLT 2020</strong> [<a href="https://aclanthology.org/2020.iwslt-1.25.pdf">paper</a>]<br> <em>Dominik Macháček, Jonáš Kratochvíl, Sangeet Sagar, Matúš Žilinec, Ondřej Bojar, Thai-Son Nguyen, Felix Schneider, Philip Williams, Yuekun Yao</em><br> <span style="font-style: italic;">The 17th International Conference on Spoken Language Translation (IWSLT 2020)</span> </div>

## Contact me

ykyao [dot] cs [at] gmail [dot] com

